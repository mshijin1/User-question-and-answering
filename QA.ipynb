{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing nessesary libraries\n",
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ACER\\OneDrive\\Documents\\ML projects\\user question and answers\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ACER\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-distilled-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# loading the pre-trained model\n",
    "model_name='distilbert-base-uncased-distilled-squad'\n",
    "model=DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer=DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# Word Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def answer_question(question, context, max_len=512):\n",
    "    # Tokenize question\n",
    "    question_tokens=tokenizer.Tokenize(tokenizer.cls_token + question + tokenizer.sep_token)\n",
    "    max_context_len=max_len-len(question_tokens)-1\n",
    "\n",
    "\n",
    "    # splitting context into chunks\n",
    "    context_tokens=tokenizer.tokenize(context)\n",
    "    chunks=[context_tokens[i:i + max_context_len] for i in range(0, len(context_tokens),max_context_len)]\n",
    "\n",
    "    best_answer=\"\"\n",
    "    best_score=float('-inf')\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # prepare inputs\n",
    "        input_tokens = question_tokens + chunk + [tokenizer.sep_token]\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "        input_ids = torch.tensor([input_ids])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)\n",
    "\n",
    "        # Extract the start and end scores for the answer\n",
    "        answer_start_scores = outputs.start_logits\n",
    "        answer_end_scores = outputs.end_logits\n",
    "\n",
    "        # find the tokens with the highest 'start' and 'end' scores\n",
    "        answer_start = torch.argmax(answer_start_scores)\n",
    "        answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "        # Convert tokens to words \n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[0][answer_start:answer_end]))\n",
    "        score = answer_start_scores[0, answer_start].item() + answer_end_scores[0, answer_end - 1].item()\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_answer = answer\n",
    "\n",
    "    return best_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Mahabharata chatbot ! Type 'exit' to end conversation.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to interact with the chatbot\n",
    "def chat_with_bot():\n",
    "    print(\"Welcome to the Mahabharata chatbot ! Type 'exit' to end conversation.\")\n",
    "    while True:\n",
    "        user_input=input(\"You: \")\n",
    "        if user_input.lower()==\"exit\":\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        else:\n",
    "            #Provide a context\n",
    "            context=\"\"\"The Mahabharata tells the story of two sets of cousins, the Pandavas and the Kauravas, whose rivalry leads to a massive conflict. It begins with King Shantanu's marriage to Ganga, who gives birth to Bhishma. Bhishma's vow of celibacy sets the stage for the unfolding drama. The central conflict arises between the Pandavas, led by Yudhishthira, and the Kauravas, led by Duryodhana. The Pandavas face trials including exile and betrayal, while the Kauravas conspire against them.\n",
    "\n",
    "The epic culminates in the great war of Kurukshetra, where the Pandavas, with Lord Krishna as their advisor, confront the Kauravas. This war represents a clash of righteousness and unrighteousness, with moral and spiritual dilemmas at its core. The Bhagavad Gita, delivered by Lord Krishna to Arjuna during the war, offers profound insights into duty and the nature of existence. Arjuna gains clarity and embraces his role as a warrior.\n",
    "\n",
    "The Mahabharata concludes with the triumph of the Pandavas, establishing dharma and justice under Yudhishthira's rule. Despite the victory, it comes at a great cost, symbolizing the complexities of human nature and the consequences of conflict. The epic marks the beginning of a new era in Bharatavarsha's history, leaving behind a legacy of wisdom and moral teachings that continue to resonate through generations.\"\"\"\n",
    "            answer=answer_question(user_input, context)\n",
    "            print(\"Chatbot:\", answer)\n",
    "# Start chatting with the bot\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
